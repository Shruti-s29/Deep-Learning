{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propogation with hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# created input vector, taking example of AND gate\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "# output \n",
    "# we have to make it 2D array and take the transpose so it becomes easy for calculation\n",
    "y = np.array([[0,0,0,1]]).T\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here z is mx+b\n",
    "def sigmoid_func(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  schema for forward propogation\n",
    "for 2 input unit (x1,x2) we are giving\n",
    " 1 - hidden layer with 2 units and 1 unit in the ouput layer\n",
    "\n",
    " - In between inp and hidden -\n",
    "  there are (4*2) = 8 random weights and 1 bias.\n",
    "   \n",
    " - In between hidden and output -\n",
    "  there are (2*1) = 2 weights and 1 bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh = 2 * np.random.random((2,2)) -1\n",
    "bh = 2 * np.random.random((1,2)) -1\n",
    "\n",
    "wo = 2 * np.random.random((2,1)) -1\n",
    "bo = 2 * np.random.random((1,1)) -1\n",
    "\n",
    "#learning rate\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output vector  [[0.47519789]\n",
      " [0.47466209]\n",
      " [0.47674599]\n",
      " [0.47458318]]\n",
      "Prediction - 0\n",
      "Prediction - 0\n",
      "Prediction - 0\n",
      "Prediction - 0\n"
     ]
    }
   ],
   "source": [
    "# main - forward propogation\n",
    "\n",
    "input = x\n",
    "out_hid = sigmoid_func(np.dot(input,wh) + bh)\n",
    "out = sigmoid_func(np.dot(out_hid,wo) + bo)\n",
    "\n",
    "print('output vector ',out)\n",
    "# for the concept of logistic regression if z >= 0.5 predict 1 , z<0.5 predict 0\n",
    "for val in out:\n",
    "    if val >= float(0.5):\n",
    "        print('Prediction - 1')\n",
    "    else:\n",
    "        print('Prediction - 0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see bad predictions hence we are going to back propogate to update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" FOR UNDERSTANDING DEEPLY above code working\\ninput = x\\nw = wh\\nb = bh\\n\\nfor _ in range(2):\\n    print('---------')\\n    print(input)\\n    print('----hid_unit1-----hid_unit2--')\\n    print(w)\\n    print(b)\\n    # summation - matrix multiplication with weights\\n    # dot product = mx\\n    summ = np.dot(input,w)\\n    # z of sigmoid function = mx + b\\n    z = summ + b\\n    out = sigmoid_func(z)\\n\\n    # updating for next iteration\\n    input = out\\n    w = wo\\n    b = bo\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' FOR UNDERSTANDING DEEPLY above code working\n",
    "input = x\n",
    "w = wh\n",
    "b = bh\n",
    "\n",
    "for _ in range(2):\n",
    "    print('---------')\n",
    "    print(input)\n",
    "    print('----hid_unit1-----hid_unit2--')\n",
    "    print(w)\n",
    "    print(b)\n",
    "    # summation - matrix multiplication with weights\n",
    "    # dot product = mx\n",
    "    summ = np.dot(input,w)\n",
    "    # z of sigmoid function = mx + b\n",
    "    z = summ + b\n",
    "    out = sigmoid_func(z)\n",
    "\n",
    "    # updating for next iteration\n",
    "    input = out\n",
    "    w = wo\n",
    "    b = bo\n",
    "'''\n",
    "# print('Output vector',out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward for Backpropogation\n",
    "- We need a cost function \n",
    "- We need a gradient descent (derivative of cost function) to update weights and biases going back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use this derivative in further calculations\n",
    "# derivative of sigmoid wrt z = sig(z)*(1- sig(z))\n",
    "def derivative_sigmoid(z):\n",
    "    return sigmoid_func(z) * (1- sigmoid_func(z))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- E = summ( (1/2) * (y_actual - y_pred)**2)\n",
    "\n",
    "\n",
    "GRADIENT -\n",
    "derivative of E wrt Wij = (dE/d oj) (d oj/d inj) (d inj/W ij) \n",
    "summation of each term**\n",
    "\n",
    "- inj - input of the hidden layer's jth unit\n",
    "- w ij - weight of ith unit to jth hidden unit\n",
    "- o j = is the output of jth hidden unit\n",
    "\n",
    "the second term is basically the derivative of sigmoid activation function.\n",
    "the third term is the input value (o i) \n",
    "\n",
    "The terms will increase as the hidden layer will increase\n",
    "\n",
    "when back propogating the third term will be weights of current layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of out  (4, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,1) and (2,1) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24764/374021362.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m## print(first_two_output_layer.shape , (wo).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mfirst_term_hid_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_two_output_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0msecond_term_hid_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mderivative_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mfirst_two_hid_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_term_hid_layer\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msecond_term_hid_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\shrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,1) and (2,1) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "for iteration in range(1000):\n",
    "    input = x\n",
    "    input_hid = np.dot(input,wh) + bh\n",
    "    out_hid = sigmoid_func(input_hid)\n",
    "    input_o = np.dot(out_hid,wo) + bo\n",
    "    out = sigmoid_func(input_o)\n",
    "    print('shape of out ',out.shape)\n",
    "\n",
    "\n",
    "    # Gradient \n",
    "    \n",
    "    first_term_output_layer = out - y\n",
    "    second_term_output_layer = derivative_sigmoid(input_o)\n",
    "    first_two_output_layer = first_term_output_layer*second_term_output_layer\n",
    "\n",
    "    print(first_two_output_layer.shape , (wo).shape)\n",
    "\n",
    "    first_term_hid_layer = np.dot(first_two_output_layer, wo)\n",
    "    second_term_hid_layer = derivative_sigmoid(input_hid)\n",
    "    first_two_hid_layer = first_term_hid_layer*second_term_hid_layer\n",
    "\n",
    "    #third term is basically features only\n",
    "\n",
    "    # CHANGING parameters\n",
    "    changes_output = np.dot(out_hid.T, first_two_output_layer)\n",
    "    changes_out_bias = np.sum(first_two_output_layer,axis=0, keepdims=True)\n",
    "    # keeping dimension cuz to ease vector addition with original bias\n",
    "\n",
    "    changes_hid = np.dot(input.T, first_two_hid_layer)\n",
    "    changes_hid_bias = np.sum(first_term_hid_layer,axis =0,keepdims=True)\n",
    "\n",
    "    # we will use these changes to update weights\n",
    "\n",
    "    # update\n",
    "    wo = wo - (lr*changes_output)\n",
    "    bo = bo - (lr*changes_hid_bias)\n",
    "\n",
    "    wh = wh - (lr*changes_hid)\n",
    "    bh = bh - (lr*changes_hid_bias)\n",
    "\n",
    "\n",
    "\n",
    "input = x\n",
    "input_hid = np.dot(input,wh) + bh\n",
    "out_hid = sigmoid_func(input_hid)\n",
    "input_o = np.dot(out_hid,wo) + bo\n",
    "out = sigmoid_func(input_o)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
